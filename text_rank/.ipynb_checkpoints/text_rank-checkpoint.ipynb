{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tweepy.api.API object at 0x0000025FF4768128>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\skena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\skena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\skena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import tweepy  #another popular twitter API wrapper\n",
    "import json\n",
    "import datetime\n",
    "import itertools, nltk, string\n",
    "import gensim\n",
    "import networkx\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "consumer_key = 'rOO0OLpyMeE27NOB0gxnoEwaq'\n",
    "consumer_secret = 'ohY5H9hg9d3u5GWcmsWyPWj0LwsQaRd0jjBeB75blpgyPuqbej'\n",
    "oauth_token = '62111881-muSYG05rnrz9DuieFcA9SeHRe8jkKuovO4jw875XY'\n",
    "oauth_token_secret = 'UTN3NYCNfbK61ezvO7a06Kj7A5myePJOAM9ujrxG2yuJV'\n",
    "\n",
    "# the authentication process for tweepy\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "\n",
    "auth.set_access_token(oauth_token, oauth_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "print(api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "page_count = 0\n",
    "for page in tweepy.Cursor(api.user_timeline, id=\"Flipkart\", tweet_mode=\"extended\").pages():\n",
    "    # page is a list of statuses\n",
    "    while 1==1:\n",
    "        try:\n",
    "            tw = page.pop()\n",
    "            if (tw.in_reply_to_status_id == None) and (tw.author.name == 'Flipkart'):\n",
    "                tweettext = str( tw.full_text.encode('ascii',errors='ignore'))\n",
    "                if tweettext.startswith(\"rt @\") == True:\n",
    "                    tweettext = tw.retweeted_status.full_text\n",
    "                text= text + tweettext + '. '\n",
    "        except IndexError:\n",
    "            break;\n",
    "            \n",
    "    page_count = page_count + 1\n",
    "    \n",
    "    if page_count > 10:\n",
    "        break\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "text\n",
    "import pandas as pd\n",
    "\n",
    "f= open(\"word_text.txt\",\"w+\", encoding='utf-8')\n",
    "f.write(text)\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun Phrase candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_chunks(text, grammar=r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'):\n",
    "    import itertools, nltk, string\n",
    "    \n",
    "    def lambda_unpack(f):\n",
    "        return lambda args: f(*args)\n",
    "    \n",
    "    # exclude candidates that are stop words or entirely punctuation\n",
    "    punct = set(string.punctuation)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    # tokenize, POS-tag, and chunk using regular expressions\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar) # punkt\n",
    "    tagged_sents = [nltk.pos_tag(nltk.word_tokenize(sent)) for sent in nltk.sent_tokenize(text)] # averaged_perceptron_tagger\n",
    "    \n",
    "    all_chunks = list(itertools.chain.from_iterable(nltk.chunk.tree2conlltags(chunker.parse(tagged_sent))\n",
    "                                                    for tagged_sent in tagged_sents))\n",
    "    # join constituent chunk words into a single chunked phrase\n",
    "    candidates = [' '.join(word for word, pos, chunk in group).lower() for key, group in\n",
    "                itertools.groupby(all_chunks, lambda_unpack(lambda word, pos, chunk: chunk != 'O')) if key]\n",
    "\n",
    "    return [cand for cand in candidates\n",
    "            if cand not in stop_words and not all(char in punct for char in cand)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'red car']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_candidate_chunks('This is a car. And I am driving it. This red car is good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_candidate_words(text, good_tags=set(['JJ','JJR','JJS','NN','NNP','NNS','NNPS'])):\n",
    "    import itertools, nltk, string\n",
    "\n",
    "    # exclude candidates that are stop words or entirely punctuation\n",
    "    punct = set(string.punctuation)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    # tokenize and POS-tag words\n",
    "    tagged_words = itertools.chain.from_iterable([nltk.pos_tag(nltk.word_tokenize(sent)) for sent in nltk.sent_tokenize(text)])\n",
    "    # filter on certain POS tags and lowercase all words\n",
    "    candidates = [word.lower() for word, tag in tagged_words\n",
    "                  if tag in good_tags and word.lower() not in stop_words\n",
    "                  and not all(char in punct for char in word)]\n",
    "\n",
    "    return candidates\n",
    "\n",
    "extract_candidate_words('This is a car. And I am driving it.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_keyphrases_by_textrank(text, n_keywords=0.05):\n",
    "    from itertools import takewhile, tee\n",
    "    import networkx, nltk\n",
    "    \n",
    "    # tokenize for all words, and extract *candidate* words\n",
    "    words = [word.lower()\n",
    "             for sent in nltk.sent_tokenize(text)\n",
    "             for word in nltk.word_tokenize(sent)]\n",
    "    candidates = extract_candidate_words(text)\n",
    "    # build graph, each node is a unique candidate\n",
    "    graph = networkx.Graph()\n",
    "    graph.add_nodes_from(set(candidates))\n",
    "    # iterate over word-pairs, add unweighted edges into graph\n",
    "    def pairwise(iterable):\n",
    "        \"\"\"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\"\"\n",
    "        a, b = tee(iterable)\n",
    "        next(b, None)\n",
    "        return zip(a, b)\n",
    "    # add the edge for pairwise\n",
    "    for w1, w2 in pairwise(candidates):\n",
    "        if w2:\n",
    "            graph.add_edge(*sorted([w1, w2]))\n",
    "    # score nodes using default pagerank algorithm, sort by score, keep top n_keywords\n",
    "    ranks = networkx.pagerank(graph)\n",
    "    # Percentage candidate words to be taken\n",
    "    if 0 < n_keywords < 1:\n",
    "        n_keywords = int(round(len(candidates) * n_keywords))\n",
    "    word_ranks = {word_rank[0]: word_rank[1]\n",
    "                  for word_rank in sorted(ranks.items(), key=lambda x: x[1], reverse=True)[:n_keywords]}\n",
    "    keywords = set(word_ranks.keys())\n",
    "    # merge keywords into keyphrases\n",
    "    keyphrases = {}\n",
    "    j = 0\n",
    "    # Create phrase\n",
    "    for i, word in enumerate(words):\n",
    "        if i < j:\n",
    "            continue\n",
    "        if word in keywords:\n",
    "            kp_words = list(takewhile(lambda x: x in keywords, words[i:i+10]))\n",
    "            avg_pagerank = sum(word_ranks[w] for w in kp_words) / float(len(kp_words))\n",
    "            keyphrases[' '.join(kp_words)] = avg_pagerank\n",
    "            # counter as hackish way to ensure merged keyphrases are non-overlapping\n",
    "            j = i + len(kp_words)\n",
    "    \n",
    "    return sorted(keyphrases.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('flipkart', 0.02679198592853726),\n",
       " ('https', 0.026106697498769496),\n",
       " ('b', 0.020157777019431396),\n",
       " ('flipkart beauty', 0.014771037864212515),\n",
       " ('flipkartpoll https', 0.014735681783379157),\n",
       " ('flipkart fashion', 0.01434511998472549),\n",
       " ('flipkart summer', 0.014333995126510096),\n",
       " ('flipkart gift', 0.014215343146262225),\n",
       " ('flipkart games', 0.014144878562073148),\n",
       " ('flipkart ceo', 0.01410765149862175),\n",
       " ('harbudgetmeinquality https', 0.013997469795136847),\n",
       " (\"b'rt\", 0.013555805668697336),\n",
       " ('flipkart fashion daily', 0.010285264256142223),\n",
       " ('amp', 0.009374377241811253),\n",
       " ('best', 0.005563743702608252),\n",
       " ('best way', 0.00447922589612437),\n",
       " ('shop', 0.00415171916439844),\n",
       " ('day', 0.004136115698079278),\n",
       " ('india', 0.004088889841113604),\n",
       " ('time', 0.004080524862356444),\n",
       " ('new', 0.004018223039174103),\n",
       " ('latest', 0.0038438128740273104),\n",
       " ('best cricket', 0.0037869076251618953),\n",
       " ('something new', 0.0037710228778331857),\n",
       " ('best quality', 0.003769940521781299),\n",
       " ('best things', 0.0036808480331768263),\n",
       " ('something', 0.0035238227164922685),\n",
       " ('latest collection', 0.0034557568003975507),\n",
       " ('way', 0.003394708089640488),\n",
       " ('flipkartpoll', 0.003364666067988819),\n",
       " ('favourite way', 0.0033101322197837783),\n",
       " ('favourite', 0.0032255563499270683),\n",
       " ('favourite brands', 0.003205204803305414),\n",
       " ('brands', 0.0031848532566837594),\n",
       " ('flipkartstories', 0.0030719177251306005),\n",
       " ('collection', 0.003067700726767791),\n",
       " (\"b'the new\", 0.003045431620761525),\n",
       " ('sure', 0.0029972229305757353),\n",
       " ('favourite beauty', 0.0029878230749074202),\n",
       " ('different', 0.002944530845573062),\n",
       " ('gameofthrones', 0.002892302430580241),\n",
       " ('great', 0.0028787464748670586),\n",
       " ('sale', 0.0028387906243716827),\n",
       " ('special day', 0.0028241332462927275),\n",
       " ('latest styles', 0.0028137483864474665),\n",
       " ('perfect', 0.0027699495983735003),\n",
       " ('beauty', 0.0027500897998877717),\n",
       " ('light', 0.002665435452960236),\n",
       " ('top brands', 0.0026267537980389543),\n",
       " ('latest accessories', 0.0026267055538189707),\n",
       " ('belikemom', 0.0025911866149309314),\n",
       " ('experience', 0.0025640928244546573),\n",
       " ('biggest brands', 0.002522035806066821),\n",
       " ('world', 0.0025089217143153954),\n",
       " ('rt', 0.0024982674034469735),\n",
       " ('great offers', 0.0024915936129692434),\n",
       " ('game', 0.0024369985946577793),\n",
       " ('big brands', 0.0023776608986730815),\n",
       " ('books', 0.002375480083002604),\n",
       " ('grooming brands', 0.002343911270385675),\n",
       " ('great things', 0.0023383494193062295),\n",
       " ('flipkarts', 0.0023218041495719015),\n",
       " ('season sale', 0.0022922212195612846),\n",
       " ('style game', 0.002290566827928882),\n",
       " ('freshonflipkart', 0.0022689842051527615),\n",
       " ('technology', 0.002266380710425916),\n",
       " ('bigshoppingdays', 0.0022372489545063826),\n",
       " ('somethingneweveryday', 0.0022245844136238994),\n",
       " ('june', 0.0022226081507139557),\n",
       " ('biggest experience', 0.00221165558995227),\n",
       " ('perfect gift', 0.002204324981180345),\n",
       " ('daily', 0.002165552798975685),\n",
       " ('biggest game', 0.002148108475053831),\n",
       " ('style', 0.002144135061199985),\n",
       " ('vote', 0.0021143601100919434),\n",
       " ('offers', 0.002104440751071428),\n",
       " ('daily life', 0.002102076863902543),\n",
       " ('products', 0.002090565768872544),\n",
       " (\"b'the\", 0.002072640202348947),\n",
       " ('top', 0.0020686543393941487),\n",
       " ('life', 0.002038600928829401),\n",
       " ('quality products', 0.002033351554913445),\n",
       " ('top quality', 0.0020223958401742476),\n",
       " ('cricket', 0.0020100715477155383),\n",
       " ('chance', 0.002008028317729968),\n",
       " ('amazing offers', 0.0019904634999899956),\n",
       " ('quality', 0.001976137340954346),\n",
       " ('right', 0.001972616016881939),\n",
       " ('happy', 0.0019470016661292063),\n",
       " ('fashion', 0.0018982540409137228),\n",
       " ('may', 0.0018884218563083948),\n",
       " ('amazing', 0.0018764862489085634),\n",
       " ('summer', 0.0018760043244829348),\n",
       " ('biggest', 0.0018592183554498828),\n",
       " ('harbudgetmeinquality challenge', 0.0018495696759917232),\n",
       " ('night', 0.001845294417060357),\n",
       " ('thanks', 0.0018165307120249813),\n",
       " ('challenge', 0.0018108972604792485),\n",
       " ('things', 0.0017979523637454),\n",
       " ('grooming products', 0.0017967675264800673),\n",
       " ('styles', 0.001783683898867623),\n",
       " ('season', 0.001745651814750886),\n",
       " ('indian', 0.0017422838315225143),\n",
       " ('youre', 0.0017206125137323516),\n",
       " ('stories', 0.0016985823509148598),\n",
       " ('copies', 0.0016605579258626448),\n",
       " ('gift', 0.0016387003639871897),\n",
       " (\"b'when youre\", 0.0016261973364385038),\n",
       " ('home', 0.001601919840404079),\n",
       " ('available', 0.0015983319436504155),\n",
       " ('camera', 0.0015940901578154103),\n",
       " ('true', 0.0015914610804796717),\n",
       " ('equal', 0.0015834463771213208),\n",
       " ('big', 0.0015704685406624035),\n",
       " ('tech', 0.0015666898922089138),\n",
       " ('bossofallfashionsales', 0.0015665423211765224),\n",
       " ('super', 0.0015602235508239046),\n",
       " ('indiakafashioncapital', 0.0015349383807179674),\n",
       " ('rajneeeshkumar', 0.0015332519889095365),\n",
       " (\"b'when\", 0.001531782159144656),\n",
       " ('special', 0.0015121507945061775),\n",
       " ('complete', 0.0015086484076279323),\n",
       " ('complete grooming', 0.0015058088458577614),\n",
       " ('grooming', 0.0015029692840875907),\n",
       " ('shark', 0.001501574628432608),\n",
       " ('rs', 0.001498567987436248),\n",
       " ('games', 0.0014977711956090383),\n",
       " ('super easy', 0.0014852209707391876),\n",
       " ('all-new', 0.0014802428086361325),\n",
       " ('workatflipkart', 0.0014743189592787691),\n",
       " ('live', 0.0014732468148326177),\n",
       " ('possible', 0.0014569232335426181),\n",
       " (\"b'thank\", 0.0014435103324575396),\n",
       " ('ceo', 0.0014233170687062442),\n",
       " ('easy', 0.0014102183906544706),\n",
       " ('accessories', 0.0014095982336106306),\n",
       " ('fire', 0.0014059685867703012),\n",
       " ('vehicles', 0.0014047848986969253),\n",
       " ('foundonflipkart', 0.0013794042797565793),\n",
       " ('look', 0.0013733895747374061),\n",
       " ('hour', 0.001356403059521642),\n",
       " ('beetroot', 0.001342071381530887)]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_keyphrases_by_textrank(text, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Rank line by line"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
