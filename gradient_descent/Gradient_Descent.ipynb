{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skena\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mxnet as mx\n",
    "print(mx.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Gradient of x =  \n",
      "[0. 0. 0. 0.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "New Gradient of x =  \n",
      "[ 8. 16. 24. 32.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "Verify that gradient hand computed = 8*x \n",
      "[ 8. 16. 24. 32.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "x = mx.nd.array([1,2,3,4])\n",
    "x.attach_grad() # Attach gradient operation\n",
    "\n",
    "with mx.autograd.record():\n",
    "    y = 2*x\n",
    "    loss = mx.nd.sum(y*y)\n",
    "    #loss function = 4 x square\n",
    "\n",
    "print (\"Current Gradient of x = \", x.grad)\n",
    "loss.backward() # Back propagation\n",
    "print (\"New Gradient of x = \", x.grad)\n",
    "print (\"Verify that gradient hand computed = 8*x\", 8*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>iris</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  iris\n",
       "0           5.1          3.5           1.4          0.2     0\n",
       "1           4.9          3.0           1.4          0.2     0\n",
       "2           4.7          3.2           1.3          0.2     0\n",
       "3           4.6          3.1           1.5          0.2     0\n",
       "4           5.0          3.6           1.4          0.2     0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris_df = pd.DataFrame(iris.data)\n",
    "iris_df.columns = ['sepal_length','sepal_width','petal_length','petal_width']\n",
    "iris_df['iris'] = iris.target\n",
    "df = iris_df\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>iris</th>\n",
       "      <th>i_setosa</th>\n",
       "      <th>i_versicolor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  iris  i_setosa  \\\n",
       "0           5.1          3.5           1.4          0.2     0         1   \n",
       "1           4.9          3.0           1.4          0.2     0         1   \n",
       "2           4.7          3.2           1.3          0.2     0         1   \n",
       "3           4.6          3.1           1.5          0.2     0         1   \n",
       "4           5.0          3.6           1.4          0.2     0         1   \n",
       "\n",
       "   i_versicolor  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create two dummy variables from \"iris\"  \n",
    "df['i_setosa'] = 0\n",
    "df.loc[(df['iris']==0), 'i_setosa']= 1\n",
    "df['i_versicolor'] = 0\n",
    "df.loc[(df['iris']==1), 'i_versicolor']= 1\n",
    "\n",
    "# Split dataset into training set and test set \n",
    "df_train, df_test = train_test_split( df, test_size=0.3, random_state=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice datasets into X (independent variables) and y (target variable)\n",
    "independent_var = ['sepal_width','petal_length','petal_width','i_setosa','i_versicolor']\n",
    "y_train = mx.nd.array(df_train['sepal_length'])\n",
    "X_train = mx.nd.array(df_train[independent_var]) \n",
    "y_test = mx.nd.array(df_test['sepal_length'])\n",
    "X_test = mx.nd.array(df_test[independent_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[1. 1. 1. 1. 1. 0.]\n",
       "<NDArray 6 @cpu(0)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.random.seed(1)\n",
    "X = mx.nd.array(X_train) \n",
    "y = mx.nd.array(y_train)\n",
    "\n",
    "\n",
    "w = [1,1,1,1,1] # Beta intercepts for dimensions\n",
    "b = [0] # intercept\n",
    "params = mx.nd.array(w + b)\n",
    "\n",
    "# Enter code here for attaching gradient\n",
    "params.attach_grad()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "#Compute func \n",
    "betatimex = X_train * params[0:5] + params[5:6]\n",
    "betatimex_sum = betatimex[:,0] + betatimex[:,1] + betatimex[:,2] + betatimex[:,3] + betatimex[:,4]\n",
    "betatimex_sum\n",
    "print (len(betatimex_sum))\n",
    "print (len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting value of params = \n",
      "[1. 1. 1. 1. 1. 0.]\n",
      "<NDArray 6 @cpu(0)>\n",
      "Gradieants of loss function wrt to params \n",
      "[16.783619  25.582096   8.73162    0.7542857  2.0266666 28.09524  ]\n",
      "<NDArray 6 @cpu(0)>\n",
      "New value of params = \n",
      "[ 0.8321638   0.744179    0.9126838   0.99245715  0.97973335 -0.2809524 ]\n",
      "<NDArray 6 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "#betatimex_sum is y hat\n",
    "# 1 itteration\n",
    "with mx.autograd.record():\n",
    "    # Enter CODE here for loss function\n",
    "    # in terms of the data and parameters w,b\n",
    "    # defined in the data_instance\n",
    "    betatimex = X * params[0:5] + params[5:6]\n",
    "    betatimex_sum = betatimex[:,0] + betatimex[:,1] + betatimex[:,2] + betatimex[:,3] + betatimex[:,4]\n",
    "    error = (y_train - betatimex_sum)\n",
    "    loss = mx.nd.mean(error*error)\n",
    "\n",
    "print ('Starting value of params = {}'.format(params))\n",
    "loss.backward()   \n",
    "#print (params)\n",
    "print ('Gradieants of loss function wrt to params {}'.format(params.grad))\n",
    "lr = 1e-2\n",
    "params = params - lr*params.grad\n",
    "print ('New value of params = {}'.format(params))\n",
    "params.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 Start \n",
      "iteration 0, Mean loss: \n",
      "[0.8541098]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 1 Start \n",
      "iteration 1, Mean loss: \n",
      "[0.7757419]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 2 Start \n",
      "iteration 2, Mean loss: \n",
      "[0.70869106]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 3 Start \n",
      "iteration 3, Mean loss: \n",
      "[0.6492679]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 4 Start \n",
      "iteration 4, Mean loss: \n",
      "[0.59660107]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 5 Start \n",
      "iteration 5, Mean loss: \n",
      "[0.5499192]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 6 Start \n",
      "iteration 6, Mean loss: \n",
      "[0.5085384]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 7 Start \n",
      "iteration 7, Mean loss: \n",
      "[0.47185385]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 8 Start \n",
      "iteration 8, Mean loss: \n",
      "[0.439329]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 9 Start \n",
      "iteration 9, Mean loss: \n",
      "[0.41048908]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 10 Start \n",
      "iteration 10, Mean loss: \n",
      "[0.38491338]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 11 Start \n",
      "iteration 11, Mean loss: \n",
      "[0.36222926]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 12 Start \n",
      "iteration 12, Mean loss: \n",
      "[0.3421064]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 13 Start \n",
      "iteration 13, Mean loss: \n",
      "[0.32425258]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 14 Start \n",
      "iteration 14, Mean loss: \n",
      "[0.30840853]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 15 Start \n",
      "iteration 15, Mean loss: \n",
      "[0.29434505]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 16 Start \n",
      "iteration 16, Mean loss: \n",
      "[0.28185877]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 17 Start \n",
      "iteration 17, Mean loss: \n",
      "[0.27076972]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 18 Start \n",
      "iteration 18, Mean loss: \n",
      "[0.26091838]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 19 Start \n",
      "iteration 19, Mean loss: \n",
      "[0.25216344]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 20 Start \n",
      "iteration 20, Mean loss: \n",
      "[0.24437982]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 21 Start \n",
      "iteration 21, Mean loss: \n",
      "[0.23745668]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 22 Start \n",
      "iteration 22, Mean loss: \n",
      "[0.23129594]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 23 Start \n",
      "iteration 23, Mean loss: \n",
      "[0.22581036]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 24 Start \n",
      "iteration 24, Mean loss: \n",
      "[0.22092305]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 25 Start \n",
      "iteration 25, Mean loss: \n",
      "[0.21656585]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 26 Start \n",
      "iteration 26, Mean loss: \n",
      "[0.21267822]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 27 Start \n",
      "iteration 27, Mean loss: \n",
      "[0.20920661]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 28 Start \n",
      "iteration 28, Mean loss: \n",
      "[0.2061035]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 29 Start \n",
      "iteration 29, Mean loss: \n",
      "[0.20332702]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 30 Start \n",
      "iteration 30, Mean loss: \n",
      "[0.2008399]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 31 Start \n",
      "iteration 31, Mean loss: \n",
      "[0.19860917]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 32 Start \n",
      "iteration 32, Mean loss: \n",
      "[0.19660568]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 33 Start \n",
      "iteration 33, Mean loss: \n",
      "[0.19480342]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 34 Start \n",
      "iteration 34, Mean loss: \n",
      "[0.19317958]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 35 Start \n",
      "iteration 35, Mean loss: \n",
      "[0.19171385]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 36 Start \n",
      "iteration 36, Mean loss: \n",
      "[0.19038822]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 37 Start \n",
      "iteration 37, Mean loss: \n",
      "[0.18918675]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 38 Start \n",
      "iteration 38, Mean loss: \n",
      "[0.18809539]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 39 Start \n",
      "iteration 39, Mean loss: \n",
      "[0.1871016]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 40 Start \n",
      "iteration 40, Mean loss: \n",
      "[0.18619438]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 41 Start \n",
      "iteration 41, Mean loss: \n",
      "[0.1853638]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 42 Start \n",
      "iteration 42, Mean loss: \n",
      "[0.18460126]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 43 Start \n",
      "iteration 43, Mean loss: \n",
      "[0.18389907]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 44 Start \n",
      "iteration 44, Mean loss: \n",
      "[0.18325034]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 45 Start \n",
      "iteration 45, Mean loss: \n",
      "[0.18264909]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 46 Start \n",
      "iteration 46, Mean loss: \n",
      "[0.18208992]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 47 Start \n",
      "iteration 47, Mean loss: \n",
      "[0.18156818]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 48 Start \n",
      "iteration 48, Mean loss: \n",
      "[0.18107963]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 49 Start \n",
      "iteration 49, Mean loss: \n",
      "[0.18062046]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 50 Start \n",
      "iteration 50, Mean loss: \n",
      "[0.18018752]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 51 Start \n",
      "iteration 51, Mean loss: \n",
      "[0.17977783]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 52 Start \n",
      "iteration 52, Mean loss: \n",
      "[0.17938878]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 53 Start \n",
      "iteration 53, Mean loss: \n",
      "[0.17901812]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 54 Start \n",
      "iteration 54, Mean loss: \n",
      "[0.17866383]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 55 Start \n",
      "iteration 55, Mean loss: \n",
      "[0.17832401]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 56 Start \n",
      "iteration 56, Mean loss: \n",
      "[0.1779972]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 57 Start \n",
      "iteration 57, Mean loss: \n",
      "[0.17768195]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 58 Start \n",
      "iteration 58, Mean loss: \n",
      "[0.17737696]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 59 Start \n",
      "iteration 59, Mean loss: \n",
      "[0.17708117]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 60 Start \n",
      "iteration 60, Mean loss: \n",
      "[0.17679355]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 61 Start \n",
      "iteration 61, Mean loss: \n",
      "[0.17651324]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 62 Start \n",
      "iteration 62, Mean loss: \n",
      "[0.17623962]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 63 Start \n",
      "iteration 63, Mean loss: \n",
      "[0.17597176]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 64 Start \n",
      "iteration 64, Mean loss: \n",
      "[0.17570926]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 65 Start \n",
      "iteration 65, Mean loss: \n",
      "[0.1754515]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 66 Start \n",
      "iteration 66, Mean loss: \n",
      "[0.17519799]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 67 Start \n",
      "iteration 67, Mean loss: \n",
      "[0.17494835]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 68 Start \n",
      "iteration 68, Mean loss: \n",
      "[0.17470212]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 69 Start \n",
      "iteration 69, Mean loss: \n",
      "[0.17445906]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 70 Start \n",
      "iteration 70, Mean loss: \n",
      "[0.17421886]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 71 Start \n",
      "iteration 71, Mean loss: \n",
      "[0.17398128]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 72 Start \n",
      "iteration 72, Mean loss: \n",
      "[0.17374602]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 73 Start \n",
      "iteration 73, Mean loss: \n",
      "[0.17351294]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 74 Start \n",
      "iteration 74, Mean loss: \n",
      "[0.1732818]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 75 Start \n",
      "iteration 75, Mean loss: \n",
      "[0.17305258]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 76 Start \n",
      "iteration 76, Mean loss: \n",
      "[0.17282492]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 77 Start \n",
      "iteration 77, Mean loss: \n",
      "[0.17259885]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 78 Start \n",
      "iteration 78, Mean loss: \n",
      "[0.17237422]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 79 Start \n",
      "iteration 79, Mean loss: \n",
      "[0.17215085]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 80 Start \n",
      "iteration 80, Mean loss: \n",
      "[0.1719288]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 81 Start \n",
      "iteration 81, Mean loss: \n",
      "[0.17170791]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 82 Start \n",
      "iteration 82, Mean loss: \n",
      "[0.1714881]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 83 Start \n",
      "iteration 83, Mean loss: \n",
      "[0.17126931]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 84 Start \n",
      "iteration 84, Mean loss: \n",
      "[0.17105153]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 85 Start \n",
      "iteration 85, Mean loss: \n",
      "[0.17083469]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 86 Start \n",
      "iteration 86, Mean loss: \n",
      "[0.1706187]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 87 Start \n",
      "iteration 87, Mean loss: \n",
      "[0.17040358]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 88 Start \n",
      "iteration 88, Mean loss: \n",
      "[0.1701893]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 89 Start \n",
      "iteration 89, Mean loss: \n",
      "[0.16997577]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 90 Start \n",
      "iteration 90, Mean loss: \n",
      "[0.169763]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 91 Start \n",
      "iteration 91, Mean loss: \n",
      "[0.16955094]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 92 Start \n",
      "iteration 92, Mean loss: \n",
      "[0.16933964]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 93 Start \n",
      "iteration 93, Mean loss: \n",
      "[0.16912903]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 94 Start \n",
      "iteration 94, Mean loss: \n",
      "[0.16891906]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 95 Start \n",
      "iteration 95, Mean loss: \n",
      "[0.16870974]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 96 Start \n",
      "iteration 96, Mean loss: \n",
      "[0.16850108]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 97 Start \n",
      "iteration 97, Mean loss: \n",
      "[0.16829301]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 98 Start \n",
      "iteration 98, Mean loss: \n",
      "[0.16808566]\n",
      "<NDArray 1 @cpu(0)>\n",
      "iteration 99 Start \n",
      "iteration 99, Mean loss: \n",
      "[0.16787884]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Mean loss')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x223136af630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Modeling parameters\n",
    "learning_rate = 1e-2\n",
    "num_iters = 100\n",
    "lr = 1e-2\n",
    "    \n",
    "\n",
    "# Use for appending training loss in each iteration\n",
    "loss_sequence = []\n",
    "    \n",
    "# Train\n",
    "for iteration in range(num_iters):\n",
    "    # Record computational graph\n",
    "    print (\"iteration %s Start \" %(iteration))\n",
    "\n",
    "\n",
    "    with mx.autograd.record():\n",
    "        # Enter CODE here for loss function\n",
    "        # in terms of the data and parameters w,b\n",
    "        # defined in the data_instance\n",
    "        betatimex = X * params[0:5] + params[5:6]\n",
    "        betatimex_sum = betatimex[:,0] + betatimex[:,1] + betatimex[:,2] + betatimex[:,3] + betatimex[:,4]\n",
    "        error = (y_train - betatimex_sum)\n",
    "        loss = mx.nd.mean(error*error)\n",
    "    loss.backward()\n",
    "\n",
    "    params = params - lr*params.grad\n",
    "    params.attach_grad()\n",
    "        \n",
    "    # Print iter, loss\n",
    "    print (\"iteration %s, Mean loss: %s\" % (iteration,loss))\n",
    "    # Append loss in each interation to loss_sequence\n",
    "    loss_sequence.append(loss.asscalar())\n",
    "        \n",
    "# Plot Training Loss    \n",
    "plt.figure(num=None,figsize=(8, 6))\n",
    "plt.plot(loss_sequence)\n",
    "plt.xlabel('iteration',fontsize=14)\n",
    "plt.ylabel('Mean loss',fontsize=14)\n",
    "\n",
    "# Test\n",
    "# Enter code here for calculating mean squared error on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Test Set: \n",
      "[0.17857154]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "betatimextest = X_test * params[0:5] + params[5:6]\n",
    "betatimex_sumtest = betatimextest[:,0] + betatimextest[:,1] + betatimextest[:,2] + betatimextest[:,3] + betatimextest[:,4]\n",
    "error_test = (y_test - betatimex_sumtest)\n",
    "MSE = mx.nd.mean(error_test*error_test)\n",
    "\n",
    "print (\"Mean Squared Error on Test Set: %s\" % (MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io, viewer, color\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage import exposure\n",
    "import pylab\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = io.imread('bw_image1.jpg', as_grey=True)  # load the image as grayscale\n",
    "print 'image matrix size: ', img.shape      # print the size of image\n",
    "print '\\n First 5 columns and rows of the image matrix: \\n', img[:5,:5]*255 \n",
    "#viewer.ImageViewer(img).show()              # plot the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(\"convolution_kernel.JPG\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve2d(image, kernel, padding=1):\n",
    "    # This function which takes an image and a kernel \n",
    "    # and returns the convolution of them\n",
    "    # Args:\n",
    "    #   image: a numpy array of size [image_height, image_width].\n",
    "    #   kernel: a numpy array of size [kernel_height, kernel_width].\n",
    "    # Returns:\n",
    "    #   a numpy array of size [image_height, image_width] (convolution output).\n",
    "    \n",
    "    kernel = np.flipud(np.fliplr(kernel))    # Flip the kernel\n",
    "    print 'image shape    ',image.shape\n",
    "    print 'kernel shape   ',kernel.shape\n",
    "    # Add zero padding to the input image\n",
    "    \n",
    "    image_padded = np.zeros((image.shape[0] + 2*padding, image.shape[1] + 2*padding))   \n",
    "    output = np.zeros((image_padded.shape[0]-kernel.shape[0] , image_padded.shape[1]-kernel.shape[0])) # convolution output\n",
    "    print 'output shape    ',output.shape\n",
    "    \n",
    "    if padding > 0:\n",
    "        image_padded[padding:-(1*padding), padding:-(1*padding)] = image\n",
    "    else:\n",
    "        image_padded = image\n",
    "    print 'image_padded shape   ',image_padded.shape\n",
    "    for x in range(image.shape[1]-kernel.shape[0]):     # Loop over every pixel of the image\n",
    "        for y in range(image.shape[0]-kernel.shape[0]):\n",
    "            # element-wise multiplication of the kernel and the image\n",
    "            output[y,x]=(kernel*image_padded[y:y+3, x:x+3]).sum()        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Relu(image, threshold):\n",
    "    image[image < threshold] = 0\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Maxpool(image, sqr_size):\n",
    "    print 'image shape    ',image.shape\n",
    "    output = np.zeros((image.shape[0]-sqr_size+1 , image.shape[1]-sqr_size+1)) # convolution output\n",
    "    print 'output shape    ',output.shape\n",
    "\n",
    "    for x in range(image.shape[1]-sqr_size):     # Loop over every pixel of the image\n",
    "        for y in range(image.shape[0]-sqr_size):\n",
    "            output[y,x]=(image_padded[y:y+sqr_size, x:x+sqr_size]).max()        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = io.imread('bw_image1.jpg')    # Load the image\n",
    "img = color.rgb2gray(img)       # Convert the image to grayscale (1 channel)\n",
    "image_equalized = exposure.equalize_adapthist(img/np.max(np.abs(img)), clip_limit=0.03)\n",
    "print '\\n First 5 columns and rows of the image_sharpen matrix: \\n', img[:5,:5]*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img, cmap=plt.cm.gray)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolve the sharpen kernel and the image\n",
    "kernel = np.array([[0,-1,0],[-1,5,-1],[0,-1,0]])\n",
    "image_sharpen = convolve2d(img,kernel, 0)\n",
    "#image_sharpen = convolve2d(image_sharpen,kernel, 0)\n",
    "print '\\n First 5 columns and rows of the image_sharpen matrix: \\n', image_sharpen[:5,:5]*255\n",
    "print '\\n New shape : \\n', img.shape\n",
    "print '\\n New shape : \\n', image_sharpen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the filtered image\n",
    "plt.imshow(image_sharpen, cmap=plt.cm.gray)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_relu = Relu(image_sharpen, 0.3)\n",
    "print '\\n First 5 columns and rows of the image_sharpen matrix: \\n', image_sharpen[:5,:5]*255\n",
    "# Plot the filtered image\n",
    "plt.imshow(image_relu, cmap=plt.cm.gray)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_maxpool = Maxpool(image_relu, 5)\n",
    "print '\\n First 5 columns and rows of the image_sharpen matrix: \\n', image_sharpen[:5,:5]*255\n",
    "# Plot the filtered image\n",
    "plt.imshow(image_sharpen, cmap=plt.cm.gray)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
